{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 02:55:06.676856: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-26 02:55:06.926890: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745607307.030428    9867 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745607307.057287    9867 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745607307.253561    9867 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745607307.253620    9867 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745607307.253622    9867 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745607307.253623    9867 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-26 02:55:07.283483: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow\n",
    "import h5py\n",
    "import numpy as np\n",
    "import chowdhury_s_model_builder\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.api.models import Sequential, Model, load_model\n",
    "from keras.api.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout, BatchNormalization, LSTM, Input, GlobalAveragePooling2D\n",
    "from keras.api.optimizers import Adam\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 256, 1292) float32\n",
      "(5000, 7) int8\n"
     ]
    }
   ],
   "source": [
    "# Open the HDF5 file in read mode\n",
    "with h5py.File(\"Data/Mid-Level_Perceptual_Features_Data.h5\", \"r\") as hf:\n",
    "    mid_level_train_data = hf[\"train\"][:]  # Load the train dataset\n",
    "    mid_level_label_data = hf[\"label\"][:]  # Load the label dataset\n",
    "\n",
    "print(mid_level_train_data.shape, mid_level_train_data.dtype)  # Check the shape and type\n",
    "print(mid_level_label_data.shape, mid_level_label_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 256, 1292) float32\n",
      "(400, 9) int8\n"
     ]
    }
   ],
   "source": [
    "# Open the HDF5 file in read mode\n",
    "with h5py.File(\"Data/Emotify_Data.h5\", \"r\") as hf:\n",
    "    emotify_train_data = hf[\"train\"][:]  # Load the train dataset\n",
    "    emotify_label_data = hf[\"label\"][:]  # Load the label dataset\n",
    "\n",
    "print(emotify_train_data.shape, emotify_train_data.dtype)  # Check the shape and type\n",
    "print(emotify_label_data.shape, emotify_label_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 256, 1292, 1) float32\n",
      "(5000, 7) int8\n"
     ]
    }
   ],
   "source": [
    "mid_level_train_data = mid_level_train_data[..., np.newaxis]\n",
    "\n",
    "print(mid_level_train_data.shape, mid_level_train_data.dtype)  # Check the shape and type\n",
    "print(mid_level_label_data.shape, mid_level_label_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 256, 1292, 1) float32\n",
      "(400, 9) int8\n"
     ]
    }
   ],
   "source": [
    "emotify_train_data = emotify_train_data[..., np.newaxis]\n",
    "\n",
    "print(emotify_train_data.shape, emotify_train_data.dtype)  # Check the shape and type\n",
    "print(emotify_label_data.shape, emotify_label_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 1292, 1) (256, 1292, 1)\n"
     ]
    }
   ],
   "source": [
    "mid_level_input_shape = mid_level_train_data.shape[1], mid_level_train_data.shape[2], mid_level_train_data.shape[3]\n",
    "emotify_input_shape = emotify_train_data.shape[1], emotify_train_data.shape[2], emotify_train_data.shape[3]\n",
    "print(mid_level_input_shape, emotify_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tensorflow.device('/GPU:0'):\n",
    "#     input = Input(shape=(256, 1292, 1))\n",
    "#     x =  Conv2D(64, (5, 5), strides=2, activation=\"relu\", padding=\"valid\")(input)\n",
    "#     x =  BatchNormalization()(x)   \n",
    "#     # 2nd Layer\n",
    "#     x =  Conv2D(64, (3, 3), strides=1, activation=\"relu\", padding=\"same\")(x)\n",
    "#     x =  BatchNormalization()(x)    \n",
    "#     # 3rd Layer\n",
    "#     x =  MaxPooling2D((2, 2))(x)\n",
    "#     x =  Dropout(0.3)(x)    \n",
    "#     # 4th Layer\n",
    "#     x =  Conv2D(128, (3, 3), strides=1, activation=\"relu\", padding=\"same\")(x)\n",
    "#     x =  BatchNormalization()(x)    \n",
    "#     # 5th Layer\n",
    "#     x =  Conv2D(128, (3, 3), strides=1, activation=\"relu\", padding=\"same\")(x)\n",
    "#     x =  BatchNormalization()(x)    \n",
    "#     # 6th Layer\n",
    "#     x =  MaxPooling2D((2, 2))(x)\n",
    "#     x =  Dropout(0.3)(x)    \n",
    "#     # 7th Layer\n",
    "#     x =  Conv2D(256, (3, 3), strides=1, activation=\"relu\", padding=\"same\")(x)\n",
    "#     x =  BatchNormalization()(x)    \n",
    "#     # 8th Layer\n",
    "#     x =  Conv2D(256, (3, 3), strides=1, activation=\"relu\", padding=\"same\")(x)\n",
    "#     x =  BatchNormalization()(x)    \n",
    "#     # 9th Layer\n",
    "#     x =  Conv2D(384, (3, 3), strides=1, activation=\"relu\", padding=\"same\")(x)\n",
    "#     x =  BatchNormalization()(x)    \n",
    "#     # 10th Layer\n",
    "#     x =  Conv2D(512, (3, 3), strides=1, activation=\"relu\", padding=\"same\")(x)\n",
    "#     x =  BatchNormalization()(x)    \n",
    "#     # 11th Layer\n",
    "#     x =  Conv2D(256, (3, 3), strides=1, activation=\"relu\", padding=\"same\")(x)\n",
    "#     x =  BatchNormalization()(x) \n",
    "#     # 12th Layer\n",
    "#     # x = tfa.layers.AdaptiveAveragePooling2D(x)\n",
    "#     x =  GlobalAveragePooling2D(keepdims=True)(x)\n",
    "#     x =  Flatten()(x)\n",
    "#     x = keras.layers.Dense(256)(x)\n",
    "#     A2Mid2E_branch = keras.layers.Dense(7, activation=\"sigmoid\")(x)\n",
    "#     model = Model(inputs=input, outputs=A2Mid2E_branch, name=\"Mid-Level_Features\")\n",
    "#     model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745607329.903951    9867 gpu_process_state.cc:208] Using CUDA malloc Async allocator for GPU: 0\n",
      "I0000 00:00:1745607329.909581    9867 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layer InputLayer True\n",
      "conv2d_4 Conv2D True\n",
      "batch_normalization_4 BatchNormalization True\n",
      "conv2d_5 Conv2D True\n",
      "batch_normalization_5 BatchNormalization True\n",
      "conv2d_6 Conv2D True\n",
      "batch_normalization_6 BatchNormalization True\n",
      "conv2d_7 Conv2D True\n",
      "batch_normalization_7 BatchNormalization True\n",
      "conv2d_8 Conv2D True\n",
      "batch_normalization_8 BatchNormalization True\n",
      "global_average_pooling2d GlobalAveragePooling2D True\n",
      "flatten Flatten True\n",
      "dense Dense True\n",
      "dropout_inserted Dropout True\n",
      "dense_1 Dense True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    if layer.trainable:\n",
    "        print(layer.name, layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1292</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">644</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">644</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">644</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">644</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">322</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">322</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">322</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">322</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">322</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">322</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">885,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,769,984</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,179,904</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ InsertedGlobalAveragePooling2D  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ InsertedDense2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ InsertedDense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,799</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m1292\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m644\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m1,664\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m644\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m644\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m644\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m322\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m322\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m322\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m322\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m322\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │       \u001b[38;5;34m147,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m322\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m384\u001b[0m)   │       \u001b[38;5;34m885,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m384\u001b[0m)   │         \u001b[38;5;34m1,536\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │     \u001b[38;5;34m1,769,984\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │     \u001b[38;5;34m1,179,904\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ InsertedGlobalAveragePooling2D  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ InsertedDense2 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ InsertedDense1 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │         \u001b[38;5;34m1,799\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,056,071</span> (19.29 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,056,071\u001b[0m (19.29 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,791,175</span> (18.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,791,175\u001b[0m (18.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">264,896</span> (1.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m264,896\u001b[0m (1.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = GlobalAveragePooling2D(name=\"InsertedGlobalAveragePooling2D\")(model.layers[-5].output)\n",
    "x = Dense(256, activation=\"relu\", name=\"InsertedDense2\")(x)\n",
    "x = Dense(7, activation=\"sigmoid\", name=\"InsertedDense1\")(x)\n",
    "model = Model(inputs=model.input, outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InsertedGlobalAveragePooling2D GlobalAveragePooling2D True\n",
      "InsertedDense2 Dense True\n",
      "InsertedDense1 Dense True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers[:-3]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "for layer in model.layers:\n",
    "    if layer.trainable:\n",
    "        print(layer.name, layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=Adam(learning_rate=0.0005),\n",
    "model.compile(optimizer=Adam(learning_rate=3.1250e-05),\n",
    "                           loss='binary_crossentropy', metrics=[\"accuracy\"]\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745607541.866878    9981 service.cc:152] XLA service 0x7f2598007ab0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1745607541.868505    9981 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2025-04-26 02:59:02.021601: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1745607542.612069    9981 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-04-26 02:59:03.626685: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1258', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-04-26 02:59:13.104588: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng11{k2=1,k3=0} for conv %cudnn-conv-bias-activation.27 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,1,256,1292]{3,2,1,0} %bitcast.6411, f32[64,1,5,5]{3,2,1,0} %bitcast.6418, f32[64]{0} %bitcast.6420), window={size=5x5 stride=2x2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:13.838632: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.468763562s\n",
      "Trying algorithm eng11{k2=1,k3=0} for conv %cudnn-conv-bias-activation.27 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,1,256,1292]{3,2,1,0} %bitcast.6411, f32[64,1,5,5]{3,2,1,0} %bitcast.6418, f32[64]{0} %bitcast.6420), window={size=5x5 stride=2x2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:19.370182: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng33{k2=2,k6=2,k13=1,k14=0,k22=0} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:21.037571: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.667511285s\n",
      "Trying algorithm eng33{k2=2,k6=2,k13=1,k14=0,k22=0} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:23.275743: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:359] gpu_async_0 cuMemAllocAsync failed to allocate 1536753664 bytes: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      " Reported by CUDA: Free memory/Total memory: 0/4294443008\n",
      "2025-04-26 02:59:23.275788: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:364] Stats: Limit:                      1853253223\n",
      "InUse:                      7267337316\n",
      "MaxInUse:                   7616627828\n",
      "NumAllocs:                         850\n",
      "MaxAllocSize:               6615040000\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-04-26 02:59:23.275821: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:68] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2025-04-26 02:59:23.275824: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 4, 33\n",
      "2025-04-26 02:59:23.275826: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 8, 6\n",
      "2025-04-26 02:59:23.275827: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 16, 3\n",
      "2025-04-26 02:59:23.275827: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 28, 5\n",
      "2025-04-26 02:59:23.275828: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 56, 1\n",
      "2025-04-26 02:59:23.275829: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 256, 10\n",
      "2025-04-26 02:59:23.275830: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 512, 10\n",
      "2025-04-26 02:59:23.275831: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 1024, 38\n",
      "2025-04-26 02:59:23.275832: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 1028, 1\n",
      "2025-04-26 02:59:23.275833: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 1536, 11\n",
      "2025-04-26 02:59:23.275833: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 2048, 11\n",
      "2025-04-26 02:59:23.275834: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 6400, 1\n",
      "2025-04-26 02:59:23.275835: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 7168, 5\n",
      "2025-04-26 02:59:23.275836: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 35000, 1\n",
      "2025-04-26 02:59:23.275837: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 147456, 1\n",
      "2025-04-26 02:59:23.275838: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 262144, 5\n",
      "2025-04-26 02:59:23.275838: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 294912, 1\n",
      "2025-04-26 02:59:23.275839: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 589824, 1\n",
      "2025-04-26 02:59:23.275840: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 1179648, 3\n",
      "2025-04-26 02:59:23.275841: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 2359296, 3\n",
      "2025-04-26 02:59:23.275842: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 3538944, 3\n",
      "2025-04-26 02:59:23.275842: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 4718592, 3\n",
      "2025-04-26 02:59:23.275843: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 7077888, 3\n",
      "2025-04-26 02:59:23.275844: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 10584064, 1\n",
      "2025-04-26 02:59:23.275845: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 16777472, 1\n",
      "2025-04-26 02:59:23.275846: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 16924672, 1\n",
      "2025-04-26 02:59:23.275847: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 182960128, 3\n",
      "2025-04-26 02:59:23.275847: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 6615040000, 1\n",
      "2025-04-26 02:59:23.275852: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:104] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 7314866176\n",
      "2025-04-26 02:59:23.275853: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:106] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 7267337316\n",
      "2025-04-26 02:59:23.275854: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:107] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 7650410496\n",
      "2025-04-26 02:59:23.275872: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:108] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 7616627828\n",
      "2025-04-26 02:59:24.277002: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng33{k2=2,k6=0,k13=1,k14=0,k22=1} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:25.409969: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.133221654s\n",
      "Trying algorithm eng33{k2=2,k6=0,k13=1,k14=0,k22=1} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:26.410336: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng11{k2=3,k3=0} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:26.578853: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.168632109s\n",
      "Trying algorithm eng11{k2=3,k3=0} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:27.579234: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng13{} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:27.802734: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.223658789s\n",
      "Trying algorithm eng13{} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:28.803424: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng11{k2=4,k3=0} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:28.882027: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.078681327s\n",
      "Trying algorithm eng11{k2=4,k3=0} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:29.882302: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng33{k2=2,k6=0,k13=2,k14=0,k22=2} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:35.885771: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 7.003566253s\n",
      "Trying algorithm eng33{k2=2,k6=0,k13=2,k14=0,k22=2} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:36.886150: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng35{k2=2,k3=0} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:37.077941: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.191948073s\n",
      "Trying algorithm eng35{k2=2,k3=0} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:38.661604: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng11{k2=0,k3=0} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:38.912427: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.250939472s\n",
      "Trying algorithm eng11{k2=0,k3=0} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:39.912758: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng15{k5=1,k6=0,k7=1,k10=4} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:41.013554: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.100924752s\n",
      "Trying algorithm eng15{k5=1,k6=0,k7=1,k10=4} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:42.013869: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng11{k2=2,k3=0} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:42.387850: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.374080156s\n",
      "Trying algorithm eng11{k2=2,k3=0} for conv %cudnn-conv-bias-activation.28 = (f32[8,64,126,644]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,126,644]{3,2,1,0} %bitcast.6446, f32[64,64,3,3]{3,2,1,0} %bitcast.6453, f32[64]{0} %bitcast.6455), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:47.092601: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng11{k2=3,k3=0} for conv %cudnn-conv-bias-activation.31 = (f32[8,256,31,161]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,128,31,161]{3,2,1,0} %bitcast.6572, f32[256,128,3,3]{3,2,1,0} %bitcast.6579, f32[256]{0} %bitcast.6581), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_4_1/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-26 02:59:47.146075: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.54948571s\n",
      "Trying algorithm eng11{k2=3,k3=0} for conv %cudnn-conv-bias-activation.31 = (f32[8,256,31,161]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,128,31,161]{3,2,1,0} %bitcast.6572, f32[256,128,3,3]{3,2,1,0} %bitcast.6579, f32[256]{0} %bitcast.6581), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_4_1/convolution\" source_file=\"/home/nigelchua0412/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "I0000 00:00:1745607593.871431    9981 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 298ms/step - accuracy: 0.4761 - loss: 0.6241\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 303ms/step - accuracy: 0.6392 - loss: 0.3888\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 291ms/step - accuracy: 0.6433 - loss: 0.2318\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 296ms/step - accuracy: 0.6599 - loss: 0.1552\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 290ms/step - accuracy: 0.6812 - loss: 0.1128\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 290ms/step - accuracy: 0.6813 - loss: 0.0818\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 293ms/step - accuracy: 0.6885 - loss: 0.0655\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 289ms/step - accuracy: 0.6907 - loss: 0.0507\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 289ms/step - accuracy: 0.7084 - loss: 0.0429\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 292ms/step - accuracy: 0.7153 - loss: 0.0362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f263db4c230>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(mid_level_train_data, mid_level_label_data,\n",
    "                       batch_size=8,\n",
    "                       epochs=10,\n",
    "                       shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"fine-tuned_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=Adam(learning_rate=0.0005),\n",
    "model.compile(optimizer=Adam(learning_rate=3.1250e-05),\n",
    "                           loss='binary_crossentropy', metrics=[\"accuracy\"]\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 293ms/step - accuracy: 0.7102 - loss: 0.0288\n",
      "Epoch 12/20\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 290ms/step - accuracy: 0.7189 - loss: 0.0243\n",
      "Epoch 13/20\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 287ms/step - accuracy: 0.7226 - loss: 0.0194\n",
      "Epoch 14/20\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 287ms/step - accuracy: 0.7178 - loss: 0.0169\n",
      "Epoch 15/20\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 291ms/step - accuracy: 0.7277 - loss: 0.0154\n",
      "Epoch 16/20\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 287ms/step - accuracy: 0.7326 - loss: 0.0135\n",
      "Epoch 17/20\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 285ms/step - accuracy: 0.7089 - loss: 0.0134\n",
      "Epoch 18/20\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 155ms/step - accuracy: 0.7194 - loss: 0.0110\n",
      "Epoch 19/20\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 276ms/step - accuracy: 0.7093 - loss: 0.0104\n",
      "Epoch 20/20\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 178ms/step - accuracy: 0.7238 - loss: 0.0096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f2634162d80>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(mid_level_train_data, mid_level_label_data,\n",
    "                       batch_size=8,\n",
    "                       epochs=20,\n",
    "                       initial_epoch=10,\n",
    "                       shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"fine-tuned_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=Adam(learning_rate=0.0005),\n",
    "model.compile(optimizer=Adam(learning_rate=3.1250e-05),\n",
    "                           loss='binary_crossentropy', metrics=[\"accuracy\"]\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 180ms/step - accuracy: 0.7318 - loss: 0.0101\n",
      "Epoch 22/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 176ms/step - accuracy: 0.7378 - loss: 0.0081\n",
      "Epoch 23/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 177ms/step - accuracy: 0.7269 - loss: 0.0079\n",
      "Epoch 24/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 177ms/step - accuracy: 0.7289 - loss: 0.0076\n",
      "Epoch 25/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 180ms/step - accuracy: 0.7332 - loss: 0.0069\n",
      "Epoch 26/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 178ms/step - accuracy: 0.7335 - loss: 0.0074\n",
      "Epoch 27/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 178ms/step - accuracy: 0.7278 - loss: 0.0067\n",
      "Epoch 28/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 180ms/step - accuracy: 0.7365 - loss: 0.0069\n",
      "Epoch 29/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 178ms/step - accuracy: 0.7375 - loss: 0.0067\n",
      "Epoch 30/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 177ms/step - accuracy: 0.7436 - loss: 0.0063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f26341f8c50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(mid_level_train_data, mid_level_label_data,\n",
    "                       batch_size=8,\n",
    "                       epochs=30,\n",
    "                       initial_epoch=20,\n",
    "                       shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"fine-tuned_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=Adam(learning_rate=0.0005),\n",
    "model.compile(optimizer=Adam(learning_rate=3.1250e-05),\n",
    "                           loss='binary_crossentropy', metrics=[\"accuracy\"]\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 177ms/step - accuracy: 0.7479 - loss: 0.0061\n",
      "Epoch 32/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 179ms/step - accuracy: 0.7423 - loss: 0.0057\n",
      "Epoch 33/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 177ms/step - accuracy: 0.7320 - loss: 0.0052\n",
      "Epoch 34/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 156ms/step - accuracy: 0.7275 - loss: 0.0058\n",
      "Epoch 35/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 172ms/step - accuracy: 0.7392 - loss: 0.0051\n",
      "Epoch 36/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 158ms/step - accuracy: 0.7425 - loss: 0.0063\n",
      "Epoch 37/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 159ms/step - accuracy: 0.7349 - loss: 0.0045\n",
      "Epoch 38/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 158ms/step - accuracy: 0.7342 - loss: 0.0042\n",
      "Epoch 39/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 159ms/step - accuracy: 0.7313 - loss: 0.0056\n",
      "Epoch 40/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 159ms/step - accuracy: 0.7436 - loss: 0.0046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f263db5d010>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(mid_level_train_data, mid_level_label_data,\n",
    "                       batch_size=8,\n",
    "                       epochs=40,\n",
    "                       initial_epoch=30,\n",
    "                       shuffle=True, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
